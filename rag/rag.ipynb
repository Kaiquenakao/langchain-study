{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7fb2a7-e183-4a57-9ecd-bf10d9f9ec49",
   "metadata": {},
   "source": [
    "Um dos aplicativos mais poderosos habilitados por LLMs são os sofisticados chatbots de perguntas e respostas (Q&A). Esses são aplicativos que podem responder perguntas sobre informações de fontes específicas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f35de6-57e2-44ad-bb22-44f921a6a897",
   "metadata": {},
   "source": [
    "# O que é RAG?\n",
    "RAG é uma técnica para aumentar o conhecimento do LLM com dados adicionais.\n",
    "\n",
    "Se você quiser construir aplicativos de IA que possam raciocinar sobre dados privados ou dados introduzidos após a data de corte de um modelo, você precisa aumentar o conhecimento do modelo com as informações específicas de que ele precisa. O processo de trazer as informações apropriadas e inseri-las no prompt do modelo é conhecido como Retrieval Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55528005-a000-4bc0-9e50-bc2d136f13d0",
   "metadata": {},
   "source": [
    "Uma aplicação RAG típica tem dois componentes principais:\n",
    "\n",
    "* <strong>Indexação</strong> : um pipeline para ingerir dados de uma fonte e indexá-los. Isso geralmente acontece offline.\n",
    "\n",
    "* <strong>Recuperação e geração</strong> : a cadeia RAG real, que recebe a consulta do usuário em tempo de execução e recupera os dados relevantes do índice, passando-os então para o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2460b-1f6a-4a54-8c16-6d4157aede45",
   "metadata": {},
   "source": [
    "## Indexação\n",
    "A sequência completa mais comum de dados brutos até a resposta se parece com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd904a-0a7e-4229-bcf6-94b3b3675a9d",
   "metadata": {},
   "source": [
    "1. Carregar : Primeiro precisamos carregar nossos dados. Isso é feito com Document Loaders .\n",
    "2. Split : divisores de texto quebram grandes Documentsem pedaços menores. Isso é útil tanto para indexar dados quanto para passá-los para um modelo, já que pedaços grandes são mais difíceis de pesquisar e não cabem na janela de contexto finita de um modelo.\n",
    "3. Store : Precisamos de um lugar para armazenar e indexar nossas divisões, para que elas possam ser pesquisadas mais tarde. Isso geralmente é feito usando um modelo VectorStore e Embeddings ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0c02f-7369-4de1-92c0-b889c29e57bf",
   "metadata": {},
   "source": [
    "## Recuperação e geração\n",
    "1. Recuperar : Dada uma entrada do usuário, as divisões relevantes são recuperadas do armazenamento usando um Retriever .\n",
    "2. Gerar : Um ChatModel / LLM produz uma resposta usando um prompt que inclui a pergunta e os dados recuperados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8625baaa-dc71-4d54-8123-e5ed365cfa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ee33a1-8bf4-44ad-ba59-3fe24074a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb02738-3c54-4f82-bd86-480c265ad850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\Kaique\\Desktop\\langchain-study\\env\\Lib\\site-packages\\langsmith\\client.py:5402: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. It often involves prompting the model to think step by step, allowing it to tackle each part systematically. Techniques like Chain of Thought (CoT) and Tree of Thoughts enhance this process by exploring various reasoning possibilities at each stage.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeff69b-d0de-4bfc-b9c4-25fbda115ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
